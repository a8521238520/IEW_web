from collections import Counter
from neo4j import GraphDatabase
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import pprint
from utils.cb_filter import *

# è¨­å®š Neo4j é€£ç·šè³‡è¨Š
NEO4J_URI = "bolt://140.116.245.146:57687"
NEO4J_USERNAME = "neo4j"
NEO4J_PASSWORD = "jimmy666666"
import pandas as pd


def split_into_sentences(text):
    """
    è‡ªè¨‚ä¸­æ–‡æ–·å¥è¦å‰‡ï¼š
    - ä»¥ä¸­æ–‡å¥è™Ÿã€é©šå˜†è™Ÿã€å•è™Ÿç‚ºåŸºç¤æ–·å¥
    - é¡å¤–å°é€—è™Ÿã€ç©ºç™½é€²è¡Œèªæ„æ–·å¥ï¼ˆä¸ç ´å£è©ï¼‰
    - ç©ºæ ¼å¾Œè‹¥ç‚ºä¸­æ–‡é–‹é ­ï¼Œå‰‡æ–·
    """
    # å…ˆä¾ä¸»è¦æ¨™é»ï¼ˆå¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿã€æ›è¡Œï¼‰åˆ†å¥
    # print("split_into_sentences:", text)
    assert isinstance(text, str), f"Expected String but got {type(text)}"
    chunks = re.split(r'([>ã€‚ï¼ï¼Ÿ!?ï¼Œâ€â€œ\n]+)|(\s*\d\.\s*)|(\s*\d\)\s*)', text)
    chunks = [chunk for chunk in chunks if chunk]
    results = []
    # print("chunks:", chunks)
    for chunk in chunks:
        chunk = chunk.strip()
        if not chunk:
            continue

        # é€²ä¸€æ­¥å°‡ chunk ä¸­çš„ä¸­æ–‡ç©ºç™½è™•æ–·å¥ï¼ˆå‰å¾Œéƒ½æ˜¯ä¸­æ–‡/æ¨™é»ï¼‰
        sub_chunks = re.split(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', chunk)
        for part in sub_chunks:
            if part.strip():
                results.append(part.strip())

    return results


class KnowledgeGraphMatcher:
    def __init__(self, uri, user, password, database, pattern_cache=None, synonym_cache=None):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.database = database
        self.pattern_cache = pattern_cache
        self.synonym_cache = {}

    def close(self):
        self.driver.close()

    def get_all_patterns(self):
        if self.pattern_cache:
            return self.pattern_cache
        """å¾ Neo4j è®€å–æ‰€æœ‰ Pattern èˆ‡ countã€type å€¼"""
        with self.driver.session(database=self.database) as session:
            result = session.run("MATCH (p:Pattern) RETURN p.type AS type, p.name AS pattern, p.count AS count")
            patterns = [{"type": r["type"], "text": r["pattern"], "count": r["count"]} for r in result]
            self.pattern_cache = patterns
            return patterns

    def get_synonyms(self, word):
        if word in self.synonym_cache:
            return self.synonym_cache[word]
        with self.driver.session(database=self.database) as session:
            result = session.run("""
                MATCH (w:Word)-[:SYNONYM_OF*1..2]->(s:Word)
                WHERE w.name = $word AND w <> s
                RETURN DISTINCT s.name AS synonym
            """, word=word)
            synonyms = [r["synonym"] for r in result]
            self.synonym_cache[word] = synonyms
            return synonyms

    def generate_regex_from_pattern(self, pattern):
        """æ ¹æ“š Pattern ç”Ÿæˆæ­£å‰‡è¡¨é”å¼"""
        words = pattern.split("_")
        regex_parts = []

        for word in words:
            synonyms = self.get_synonyms(word)
            synonyms.append(word)  # åŒ…å«è‡ªå·±
            synonyms = list(set(synonyms))
            regex_parts.append(f"(?:{'|'.join(map(re.escape, synonyms))})")
        # print(regex_parts)
        return ".*?".join(regex_parts)

    def match_text_against_patterns(self, input_text, window_size=1):
        """å°‡è¼¸å…¥æ–‡æœ¬åˆ†å¥å¾Œï¼Œæ»‘å‹•å…©å¥ä¸€èµ·æ¯”å° pattern"""
        sentences = split_into_sentences(input_text)
        sentences = [s for s in sentences if s not in {'ï¼Œ', 'ã€‚'}]
        """æª¢æŸ¥è¼¸å…¥æ–‡å­—æ˜¯å¦ç¬¦åˆ Neo4j ä¸­çš„ä»»ä½• Pattern"""
        patterns = self.get_all_patterns()
        matched_results = []
        window_size2_pattern = {'ä»–äºº_æˆ‘_å¤±å»', "å¤±å»_ä»–äºº_æˆ‘_èµ°ä¸å‡ºä¾†", "æˆ‘_æ²’æœ‰å‹‡æ°£_é›¢é–‹", "æ²’æœ‰_ä»–äºº_æ€éº¼_å­˜æ´»",
                                "å¤©ç”Ÿéº—è³ª_æ‰èƒ½_å—åˆ°_çŸšç›®", "åŠªåŠ›_å»_å˜²è«·", "æœŸè¨±_åš_æ´»èºäºº",
                                "ä»€éº¼éƒ½æƒ³åšåˆ°_ä»€éº¼éƒ½æƒ³åšå¥½", "æ‰“ç ´èª“è¨€_ç„¡æ³•å®Œæˆ_è¨ˆåŠƒ", "ä¸èƒ½_æˆç‚º_ç„¦é»_è®“æˆ‘å¿ƒç¢",
                                "ä¸é¡§_ä»–äºº_è‡ªå·±_ç‚ºä¸»", "ä½ æœƒå¤±å»å¦³çš„å·¥ä½œ_å› ç‚ºæˆ‘", "ä¸€ç›´_èº²æˆ¿é–“_æ²’åœ¨æ‰¿æ“”_è²¬ä»»",
                                "é‡åˆ°_å¥½åƒ_ç„¡æ³•å¿è€_äº‹å¾Œ_å‚¬å", "ä¸æƒ³åƒè—¥_å°±_å‡è£å¿˜", "æ²’æœ‰å¥½å¥½åƒè—¥_éƒ½äº‚åƒ",
                                "æä¸èµ·å‹_åšäº‹æƒ…_ä¸æƒ³_èµ·åºŠ", "è¦_ä¸Šèª²_ä½†_æˆ‘_æ²’å‹•åŠ›", "çŸ¥é“_è©²åšä»€éº¼_å°±æ˜¯_é€ƒé¿",
                                "ä»€éº¼äº‹éƒ½ä¸æƒ³åš_åªæƒ³_æ²‰æ·ª", "çŸ¥é“_æœ‰å¾ˆå¤šäº‹_è¦åš_ä½†_ä¸æƒ³å»åš", "äººéš›é—œä¿‚_ä¸æ‡‚",
                                "åƒ_æ€•èƒ–_åå‡ºä¾†", "ä¿¡è³´_è¢«_æ’•ç¢", "å¤–åœ¨_çª’æ¯", "é€ƒåˆ°_æ²’æœ‰äºº_çš„åœ°æ–¹",
                                "è€å­_æ”¾æ£„_åªè¦_é–‹_è—¥_å°±å¯ä»¥", "ä¸å–œæ­¡_ä½†å¯ä»¥æ¥å—", "æœ‰_äº‹æƒ…æƒ³åš_ä½†æ˜¯_æƒ³_ä¼‘æ¯","è¢«å‚·å®³_é‚„_æ“”å¿ƒä»–_æœƒä¸æœƒ_å—å‚·"
                                "æ‹œè¨—_ä»–äºº_è·Ÿæˆ‘èŠå¤©", "ä»–äºº_é™ªä¼´_æˆ‘_å£“åŠ›_å¤§", "æ€éº¼_æ‰_ä¸éº»ç…©_ä»–äºº","ä¸æƒ³_é€ æˆ_è² æ“”_å‡è£_æ²’æœ‰_ç•°å¸¸","ä¿¡ä»»_ä»–äºº_æ‹¿æˆ‘_ç•¶æ“‹ç®­ç‰Œ", "è¦å‘Šè¨´è‡ªå·±_å¾ˆæ£’_å¯æ˜¯_æˆ‘_ç„¡æ³•","è‡ªæˆ‘åƒ¹å€¼_å´©å¡Œ", "çŸ¥é“_ç†¬å¤œ_æœƒ_æ‰é€²é»‘æ´_æˆ‘é‚„æ˜¯åš", "èº«è€Œç‚ºäºº_å¾ˆæŠ±æ­‰"}
        i = 0
        while i < len(sentences):
            matched = False
            for pattern_data in patterns:
                pattern = pattern_data["text"]

                # è‹¥æ­¤ pattern ç‚ºæŒ‡å®š patternï¼Œå‰‡ä½¿ç”¨ window_size = 2
                current_window_size = 2 if pattern in window_size2_pattern else window_size

                if i + current_window_size > len(sentences):
                    continue  # é¿å…è¶…å‡ºç¯„åœ

                combined = "ã€‚".join(sentences[i:i + current_window_size])
                regex_pattern = self.generate_regex_from_pattern(pattern)

                if re.search(regex_pattern, combined, re.IGNORECASE):
                    matched_results.append([pattern_data, combined])
                    matched = True
                    i += current_window_size  # è‹¥æœ‰ matchï¼Œè·³éæ•´å€‹ window_size
                    break  # ä¸€æ—¦æœ‰åŒ¹é…ï¼Œåœæ­¢æª¢æŸ¥æ­¤è¦–çª—ä¸­çš„å…¶ä»– pattern

            if not matched:
                i += 1  # æ²’ match å°±ç…§èˆŠæ»‘å‹•ä¸€æ ¼

        return matched_results


def cb_match_threaded(test_cases, database):
    """å–®ä¸€è³‡æ–™åº«çš„æ¯”å°ä»»å‹™ï¼ˆç”¨æ–¼åŸ·è¡Œç·’ï¼‰"""
    kg_matcher = KnowledgeGraphMatcher(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, database)
    matched_patterns = kg_matcher.match_text_against_patterns(test_cases)
    kg_matcher.close()
    return matched_patterns


pattern_cache_dict = {}
synonym_cache_dict = {}


def init_db_cache(db):
    # print(f"ğŸ” åˆå§‹åŒ– {db} ä¸­...")
    kg = KnowledgeGraphMatcher(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, db)
    pattern_cache = kg.get_all_patterns()
    synonym_cache = {}
    for pat in pattern_cache:
        for word in pat["text"].split("_"):
            if word not in synonym_cache:
                synonym_cache[word] = kg.get_synonyms(word)
    kg.close()
    pattern_cache_dict[db] = pattern_cache
    synonym_cache_dict[db] = synonym_cache
    # print(f"âœ… {db} å®Œæˆ")


def get_cached_matcher(db):
    return KnowledgeGraphMatcher(
        NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, db,
        pattern_cache=pattern_cache_dict[db],
        synonym_cache=synonym_cache_dict[db]
    )


def single_db_match(database, text: str):
    matcher = get_cached_matcher(database)
    matches = matcher.match_text_against_patterns(text)
    # print(matches)
    matcher.close()
    if matches:
        print(f"âœ… {database} åŒ¹é…åˆ° {len(matches)} ç­† ")
    return matches


def event_matches(text: str):
    # print("æ­£åœ¨åˆå§‹åŒ– pattern èˆ‡åŒç¾©è©å¿«å–...")
    databases = ["cba", "cbb", "cbc", "cbd", "cbe", "cbf", "cbg", "cbh", "cbi", "cbj", "cbk", "cbl", "cbm"]

    # print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰è³‡æ–™åº«çš„ pattern/synonym å¿«å–...")
    with ThreadPoolExecutor(max_workers=len(databases)) as executor:
        futures = [executor.submit(init_db_cache, db) for db in databases]
        for future in futures:
            future.result()
    # print("âœ… æ‰€æœ‰è³‡æ–™åº«å¿«å–åˆå§‹åŒ–å®Œæˆï¼")
    # é å…ˆæŠ“å– pattern èˆ‡ synonymï¼ˆåªéœ€å¾ä¸€å€‹è³‡æ–™åº«ä¸­å–å¾—ï¼‰
    all_results = []

    # ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡Œå¤šåŸ·è¡Œç·’ä»»å‹™
    type_to_sentences = defaultdict(list)

    with ThreadPoolExecutor(max_workers=len(databases)) as executor:
        futures = {executor.submit(single_db_match, db, text): db for db in databases}
        for future in futures:
            matches = future.result()
            if matches:
                for pattern_info, matched_sentence in matches:
                    type_to_sentences[pattern_info["type"]].append((matched_sentence, pattern_info["text"]))

    result = {}

    for label in sorted(type_to_sentences.keys()):
        # print(f"{label}: {type_to_sentences[label]}")
        result[label] = list(set(type_to_sentences[label]))  # å»é‡
    for label in [chr(c) for c in range(ord('A'), ord('N'))]:
        if label not in result:
            result[label] = []
    all_results.append(result)
    pprint.pprint(all_results)
    return all_results


def collect_all_patterns(database):
    # databases = ["cba", "cbb", "cbc", "cbd", "cbe", "cbf", "cbg", "cbh", "cbi", "cbj", "cbk", "cbl", "cbm"]
    all_patterns = set()

    kg = KnowledgeGraphMatcher(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, database)
    patterns = kg.get_all_patterns()
    kg.close()

    for pat in patterns:
        all_patterns.add(pat["text"])  # åªå– pattern æ–‡å­—
        # è½‰æˆ dict ä¸¦åˆå§‹åŒ–ç‚º 0
    pattern_dict = {p: 0 for p in all_patterns}
    return pattern_dict

def cb_extraction(input_text: str):
    # æŒ‡å®š A~M é¡åˆ¥
    cb_types = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']
    events = event_matches(input_text)
    # å–å‡ºç¬¬ä¸€ç¯‡æ–‡ç« çš„ A~M list
    cb_lists = {key: events[0][key] for key in cb_types}
    print("cb_lists:", cb_lists)
    return cb_lists

def cb_pattern_extractor(input_text: str):
    cb_extract = cb_extraction(input_text)
    cb_extract = filter_event_A_dict(cb_extract)
    cb_extract = filter_event_B_dict(cb_extract)
    cb_extract = filter_event_C_dict(cb_extract)
    cb_extract = filter_event_D_dict(cb_extract)
    cb_extract = filter_event_E_dict(cb_extract)
    cb_extract = filter_event_F_dict(cb_extract)
    cb_extract = filter_event_G_dict(cb_extract, input_text)
    cb_extract = filter_event_H_dict(cb_extract)
    cb_extract = filter_event_I_dict(cb_extract)
    cb_extract = filter_event_J_dict(cb_extract)
    cb_extract = filter_event_K_dict(cb_extract)
    cb_extract = filter_event_L_dict(cb_extract)
    cb_extract = filter_event_M_dict(cb_extract)
    return cb_extract

if __name__ == "__main__":
    # pprint.pprint(collect_all_patterns())
    res = cb_pattern_extractor(
        "ç¤¾ç• æ­·ç¨‹ åœ‹å°ç”šè‡³å¹¼ç¨šåœ’é–‹å§‹ï¼Œå› ç‚ºå°è°æ˜ï¼Œç¸½æ˜¯è¢«å°å¸«é¸ç‚ºç­é•·ä¹‹é¡çš„è¨äººå­çš„è§’è‰²ï¼Œæˆ‘è¨å­èˆ‡çœ¾ä¸åŒæˆ‘ä¸æ˜¯ä¸»æµçš„é‚£ç¨®ç¤¾äº¤å’–ï¼Œæˆ‘é•·å¾—å¾ˆé†œå¥³ç”Ÿå«‰å¦’æˆç¸¾å¥½ã€ç”·ç”Ÿä¸æŠŠæˆ‘ç•¶å¥³ç”Ÿçœ‹ï¼Œæˆ‘è¨å­ä¸Šå°ç®¡ç§©åºï¼Œå› ç‚ºåƒæˆ‘é€™ç¨®æˆç¸¾å„ªç•°çš„å»–æ‰’ä»”å¥½åƒä¸æ‡‚æ°‘é–“ç–¾è‹¦ä¸€æ¨£ï¼Œè€å¸«ä»€éº¼éƒ½è½æˆ‘çš„ï¼ŒåŒå­¸æŠŠæˆ‘çš„è©±ç•¶è€³é‚Šé¢¨ï¼Œéœ¸å‡Œå¾æ­¤é–‹å§‹ï¼ŒæŠ—æ‹’ä¸Šå°ï¼Œåˆ°é ˜å®Œå¸‚é•·çç•¢æ¥­ã€‚ æŸå¸«ï¼šå¸Œæœ›ä½ å¯ä»¥ä»¥Kå¥³ç‚ºç›®æ¨™ å°å…­ï¼šç‚ºä»€éº¼ï¼Ÿ(æˆ‘é€£åœ‹ä¸­éƒ½é‚„æ²’é¸å¥½) æŸå¸«ï¼šå› ç‚ºä½ æˆç¸¾å¾ˆå¥½ï¼Œé€™æ¨£æ‰å¯ä»¥è€ƒä¸Šå¥½å¤§å­¸ å°å…­ï¼šå¥½å¤§å­¸è¦å¹¹å˜› æŸå¸«ï¼šé€™æ¨£ä½ æ‰èƒ½æ‰¾åˆ°å¥½å·¥ä½œè½‰å¤§éŒ¢ å°å…­ï¼šå°±é€™æ¨£ï¼Ÿï¼Ÿï¼Ÿ å½¼æ™‚æˆ‘æ²’æœ‰ä»»ä½•å¤¢æƒ³ï¼Œåˆ¥äººæƒ³è¦ç•¶é†«ç”Ÿã€ç•«å®¶ã€æ‹¯æ•‘ä¸–ç•Œã€è²·æˆ¿å­çµ¦çˆ¸åª½ã€ç’°éŠä¸–ç•Œã€çµå©šã€åšç ”ç©¶å¤ªå¤šäº† å®¶åº­èƒŒæ™¯ä½¿æˆ‘æ²’æœ‰ä»»ä½•å¤¢æƒ³ä»¥åŠæ•¢åšå¤¢çš„èƒ½åŠ›ï¼Œæ¯è¦ªä¸€å¿ƒä¸€æ„è¦å…¨éƒ¨å°å­©å»è€ƒä»–é‚£é¤Šæ´»ä¸€å®¶å­çš„åœ‹ç‡Ÿäº‹æ¥­ï¼Œåªæƒ³è‘—é‚£å°±è€ƒä¸ŠKå¥³å§ï¼Œç„¶å¾Œè€ƒPå¤§ï¼Œä¸€è¼©å­ä¸æ„åƒä½å·¥ä½œã€‚ ä¸€æ–¹é¢ä¹Ÿåªæ˜¯ç‚ºäº†æ»¿è¶³æ¯è¦ªæœªå®Œæˆçš„å­¸æ­·çš„æ›¿ä»£å“ç½·äº†ï¼Œä¹Ÿè¨±å¿µäº†Kå·¥ï¼Œäººç”Ÿå°±æœƒä¸åŒäº† ï¼Œä¸é‡è¦ ä¸€æ–¹é¢è¦ºå¾—åœ¨äºŒä¸‰å¿—é¡˜ç„¡æ³•æŠ‰æ“‡æ™‚ï¼Œè€ƒç¬¬ä¸€å¿—é¡˜å°±æ²’é€™éº¼ç…©æƒ±äº†ã€‚ äººç”Ÿå°±æ˜¯é€™æ¨£ï¼Œæˆ‘å°±æ˜¯å°ç£æ•™è‚²é«”åˆ¶ä¸‹çš„æ•—é¡ï¼Œé¢å°æ–°å·¥ä½œå‰è¼©æ¯”è‡ªå·±å°å¤šå¹´ï¼Œèƒ½åŠ›åˆå¼·å¯¦åœ¨æ…šæ„§å£“åŠ›å¤§ã€‚ å¤§å­¸èƒ½é¸è€ƒè©¦èª²å°±ä¸é¸å ±å‘Šèª²ï¼Œåœ¨ç³»ä¸ŠåŒå­¸çœ¼è£¡ä¹Ÿæ˜¯åè¶³æ€ªå’–ï¼Œç›´åˆ°ç ”ç©¶æ‰€ï¼Œæ¨äº†ä¸€æ‰€åªçœ‹æ›¸å¯©çš„ç³»ï¼Œå†ä¹Ÿæ²’æœ‰ç¹¼çºŒèº²åœ¨å­¸æ ¡çš„ç†ç”±ï¼Œæˆ‘å®³æ€•å£è©¦åˆ¥å¿˜äº†ï¼Œæƒ³åˆ°è«–æ–‡å¯«å®Œè¦å£è©¦æˆ‘é‚„æ²’ä¸‹æ‰‹è«–æ–‡æ™‚å°±å…ˆé€€å­¸äº†ï¼Œé€™è¼©å­ä¸€ç›´åœ¨ç”¨æ¿€çƒˆé€ƒé¿çš„æ–¹å¼å’Œæˆ‘çš„ç¤¾ç•ç‚ºä¼ã€‚ ä¸Šç­é–‹å§‹ï¼Œå› ç‚ºå…§æ¥­éœ€è¦æ¥é›»è©±ï¼Œæˆ‘ä»ç„¶å¾ˆæŠ—æ‹’ï¼Œæ²’äººåœ¨çš„è©±æˆ‘æœƒè®“å®ƒéŸ¿åˆ°åœï¼Œç‚ºæ­¤è¢«ç½µå¾ˆå¤šæ¬¡ï¼Œé˜¿å°±ä¸æ˜¯æ‰¾æˆ‘é˜¿ï¼Œä½ è¦æ‰¾é˜¿äººåˆä¸åœ¨æˆ–ä¸æ¥ï¼Œé›»è©±ã€ç°¡å ±ã€é¢è©¦ï¼Œå¤§æ¦‚æ˜¯ç¤¾ç•å°æˆ‘é€ æˆæœ€å¤§å›°æ“¾ã€‚ å¤šå¹´å‰åŒ—é†«ä¸»ä»»æœ‰å•æˆ‘è¦ä¸è¦åƒåŠ å¿ƒç†æ²»ç™‚å·¥ä½œåŠï¼Œå› ç‚ºæ‡¶æŠ—æ‹’å›çµ•äº†ï¼Œçªç„¶æƒ³è¦é¢å°ä¸€ä¸‹ æ–°å·¥ä½œä¸€é€±ï¼Œæ¯å¤©èƒƒç³¾çµã€èƒ¸å£ç¼ç†±ã€æ‰‹æ±—ç›´æµï¼Œçµ¦è‡ªå·±ä¸€å€‹æœˆï¼Œåªæ˜¯å®¤å‹èªªå¤ªçŸ­äº†è¦å…©å€‹æœˆã€‚ å‰è¼©æ¯å¤©7AMä¸Šç­11PMä¸‹ç­ï¼Œæº–æ™‚ä¸‹ç­çš„æˆ‘å…§å¿ƒçœŸçš„éä¸å» é›–ç„¶æˆ‘ä»–åª½çš„æ²’æœ‰æƒ³å¤šç•™ä¸€åˆ» ï¼Œä¸‹ç­ï¼Œå†è¦‹ã€‚ å»ºç«‹ç”Ÿæ´»ï¼Œå¾åƒè—¥ã€é‹å‹•ã€è¡£æ«ƒæ›å­£ã€æ‹–åœ°ã€æ´—åºŠå–®ã€æ›¬æ£‰è¢«ã€ç¿»æ›¸å‡ºä¾†çœ‹é–‹å§‹ï¼Œæ˜æ˜ä¸‹ç­æ™‚é–“å¾ˆå¤šï¼Œå»å¸¸å¸¸æ»‘æ‰‹æ©Ÿåˆ°åŠå¤œï¼Œå¥½å¥½ç”Ÿæ´»å¥½é›£ï¼Œé€£å¿ƒè‡Ÿä¸è¦äº‚è·³ã€æ‰‹è…³åœæ­¢æµæ±—é–‹å§‹ã€‚ å¸Œæœ›æœ‰äººå¯ä»¥è¿´éŸ¿ï¼Œç»ç’ƒå¿ƒæ±‚æ‹æ‹ åƒäº†å…©é¡†å®‰çœ è—¥é‚„æ¸…é†’ï¼Œæ…˜ã€‚")
    print(res)

